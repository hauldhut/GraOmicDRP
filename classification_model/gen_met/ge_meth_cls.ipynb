{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ge_meth_cls.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"CF2VsH5siZVp"},"source":["!pip install torch==1.4.0+cu100 torchvision==0.5.0+cu100 -f https://download.pytorch.org/whl/torch_stable.html\n","!pip install torch-geometric \\\n","  torch-sparse==latest+cu100 \\\n","  torch-scatter==latest+cu100 \\\n","  torch-cluster==latest+cu100 \\\n","  -f https://pytorch-geometric.com/whl/torch-1.4.0.html"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HX7n61f853zw"},"source":["\n","import numpy as np\n","import pandas as pd\n","import sys, os\n","from random import shuffle\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn import Sequential, Linear, ReLU\n","from torch_geometric.nn import GINConv, global_add_pool\n","from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n","from models.gat import GATNet\n","from models.gat_gcn import GAT_GCN\n","from models.gcn import GCNNet\n","from models.ginconv import GINConvNet\n","from utils import FocalLoss\n","from utils import *\n","import datetime\n","import argparse\n","\n","store_path = \"/content/drive/MyDrive/05.New_DRP3_ge_meth/Pretrain_DRP/DRP_RS_ge_meth/\"\n","data_path = \"/content/drive/MyDrive/05.New_DRP3_ge_meth/recall_moli/create_data/Splitting_data_new/ge_meth/\" # data to process\n","data_processed_path = \"/content/drive/MyDrive/05.New_DRP3_ge_meth/recall_moli/create_data/Splitting_data_new/ge_meth/processed/\" # data processed\n","pretrain_model_path = \"/content/drive/MyDrive/05.New_DRP3_ge_meth/Pretrain_DRP/DRP_RS_ge_meth/model_GINconvNet_GDSC_ge_meth_new.model\" # weight of the model (GIN)\n","# GINConv model\n","class GINConvNet(torch.nn.Module):\n","    def __init__(self, n_output=1,num_features_xd=78, num_features_xt=25,\n","                 n_filters=32, embed_dim=128, output_dim=128, dropout=0.2):\n","\n","        super(GINConvNet, self).__init__()\n","\n","        dim = 32\n","        self.dropout = nn.Dropout(dropout)\n","        self.relu = nn.ReLU()\n","        self.n_output = n_output\n","        # convolution layers\n","        nn1 = Sequential(Linear(num_features_xd, dim), ReLU(), Linear(dim, dim))\n","        self.conv1 = GINConv(nn1)\n","        self.bn1 = torch.nn.BatchNorm1d(dim)\n","\n","        nn2 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n","        self.conv2 = GINConv(nn2)\n","        self.bn2 = torch.nn.BatchNorm1d(dim)\n","\n","        nn3 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n","        self.conv3 = GINConv(nn3)\n","        self.bn3 = torch.nn.BatchNorm1d(dim)\n","\n","        nn4 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n","        self.conv4 = GINConv(nn4)\n","        self.bn4 = torch.nn.BatchNorm1d(dim)\n","\n","        nn5 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n","        self.conv5 = GINConv(nn5)\n","        self.bn5 = torch.nn.BatchNorm1d(dim)\n","\n","        self.fc1_xd = Linear(dim, output_dim)\n","\n","        # 1D convolution on protein sequence  # mut features\n","        self.embedding_xt_mut = nn.Embedding(num_features_xt + 1, embed_dim)\n","        self.conv_xt_mut_1 = nn.Conv1d(in_channels=1000, out_channels=n_filters, kernel_size=8)\n","\n","        # 1D convolution on protein sequence # mut features\n","        self.embedding_xt_meth = nn.Embedding(num_features_xt + 1, embed_dim)\n","        self.conv_xt_meth_1 = nn.Conv1d(in_channels=1000, out_channels=n_filters, kernel_size=8)\n","\n","\n","        # cell line mut feature\n","        self.conv_xt_mut_1 = nn.Conv1d(in_channels=1, out_channels=n_filters, kernel_size=8)\n","        self.pool_xt_mut_1 = nn.MaxPool1d(3)\n","        self.conv_xt_mut_2 = nn.Conv1d(in_channels=n_filters, out_channels=n_filters*2, kernel_size=8)\n","        self.pool_xt_mut_2 = nn.MaxPool1d(3)\n","        self.conv_xt_mut_3 = nn.Conv1d(in_channels=n_filters*2, out_channels=n_filters*4, kernel_size=8)\n","        self.pool_xt_mut_3 = nn.MaxPool1d(3)\n","        self.fc1_xt_mut = nn.Linear(1280, output_dim)\n","\n","        # cell line meth feature\n","        self.conv_xt_meth_1 = nn.Conv1d(in_channels=1, out_channels=n_filters, kernel_size=8)\n","        self.pool_xt_meth_1 = nn.MaxPool1d(3)\n","        self.conv_xt_meth_2 = nn.Conv1d(in_channels=n_filters, out_channels=n_filters*2, kernel_size=8)\n","        self.pool_xt_meth_2 = nn.MaxPool1d(3)\n","        self.conv_xt_meth_3 = nn.Conv1d(in_channels=n_filters*2, out_channels=n_filters*4, kernel_size=8)\n","        self.pool_xt_meth_3 = nn.MaxPool1d(3)\n","        self.fc1_xt_meth = nn.Linear(83584, output_dim)\n","\n","        # combined layers\n","        self.fc1 = nn.Linear(3*output_dim, 1024)\n","        self.fc2 = nn.Linear(1024, 128)\n","        self.out = nn.Linear(128, n_output)\n","\n","        # activation and regularization\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(0.5)\n","\n","    def forward(self, data):\n","        x, edge_index, batch = data.x, data.edge_index, data.batch\n","        #print(x)\n","        #print(data.target)\n","        x = F.relu(self.conv1(x, edge_index))\n","        x = self.bn1(x)\n","        x = F.relu(self.conv2(x, edge_index))\n","        x = self.bn2(x)\n","        x = F.relu(self.conv3(x, edge_index))\n","        x = self.bn3(x)\n","        x = F.relu(self.conv4(x, edge_index))\n","        x = self.bn4(x)\n","        x = F.relu(self.conv5(x, edge_index))\n","        x = self.bn5(x)\n","        x = global_add_pool(x, batch)\n","        x = F.relu(self.fc1_xd(x))\n","        x = F.dropout(x, p=0.2, training=self.training)\n","\n","        # protein input feed-forward:\n","        target_mut = data.target_mut\n","        #print(data.target_mut)\n","        #print(len(data.target_mut))\n","        #print(len(data.target_mut[1]))\n","        target_mut = target_mut[:,None,:]\n","\n","        # protein input feed-forward:\n","        target_meth = data.target_meth\n","        #print(data.target_meth)\n","        #print(len(data.target_meth))\n","        #print(len(data.target_meth[1]))\n","        target_meth = target_meth[:,None,:]\n","\n","\n","       \n","        # 1d conv layers xt_mut\n","        conv_xt_mut = self.conv_xt_mut_1(target_mut)\n","        conv_xt_mut = F.relu(conv_xt_mut)\n","        conv_xt_mut = self.pool_xt_mut_1(conv_xt_mut)\n","        conv_xt_mut = self.conv_xt_mut_2(conv_xt_mut)\n","        conv_xt_mut = F.relu(conv_xt_mut)\n","        conv_xt_mut = self.pool_xt_mut_2(conv_xt_mut)\n","        conv_xt_mut = self.conv_xt_mut_3(conv_xt_mut)\n","        conv_xt_mut = F.relu(conv_xt_mut)\n","        conv_xt_mut = self.pool_xt_mut_3(conv_xt_mut)\n","        \n","        # 1d conv layers\n","        conv_xt_meth = self.conv_xt_meth_1(target_meth)\n","        conv_xt_meth = F.relu(conv_xt_meth)\n","        conv_xt_meth = self.pool_xt_meth_1(conv_xt_meth)\n","        conv_xt_meth = self.conv_xt_meth_2(conv_xt_meth)\n","        conv_xt_meth = F.relu(conv_xt_meth)\n","        conv_xt_meth = self.pool_xt_meth_2(conv_xt_meth)\n","        conv_xt_meth = self.conv_xt_meth_3(conv_xt_meth)\n","        conv_xt_meth = F.relu(conv_xt_meth)\n","        conv_xt_meth = self.pool_xt_meth_3(conv_xt_meth)\n","\n","\n","        # flatten mut\n","        xt_mut = conv_xt_mut.view(-1, conv_xt_mut.shape[1] * conv_xt_mut.shape[2])\n","        xt_mut = self.fc1_xt_mut(xt_mut)\n","        #print(xt_mut)\n","        \n","        # flatten meth\n","        xt_meth = conv_xt_meth.view(-1, conv_xt_meth.shape[1] * conv_xt_meth.shape[2])\n","        xt_meth = self.fc1_xt_meth(xt_meth)\n","        #print(xt_meth)\n","        \n","\n","        # concat\n","        xc = torch.cat((x, xt_mut, xt_meth), 1)\n","        # add some dense layers\n","        xc = self.fc1(xc)\n","        xc = self.relu(xc)\n","        xc = self.dropout(xc)\n","        xc = self.fc2(xc)\n","        xc = self.relu(xc)\n","        xc = self.dropout(xc)\n","        out = self.out(xc)\n","        return out, x\n","from sklearn.metrics import roc_auc_score\n","def roc_auc_compute_fn(y_preds: torch.Tensor, y_targets: torch.Tensor) -> float:\n","    y_true = y_targets.numpy()\n","    y_pred = y_preds.numpy()\n","    return roc_auc_score(y_true, y_pred)\n","\n","def set_parameter_requires_grad(model, feature_extracting=True):\n","    if feature_extracting:\n","        for param in model.parameters():\n","            param.requires_grad = False\n","\n","# training function at each epoch\n","def train(model, device, train_loader, optimizer, epoch, log_interval):\n","    print('Training on {} samples...'.format(len(train_loader.dataset)))\n","    model.train()\n","    loss_fn = nn.BCELoss()\n","    avg_loss = []\n","    for batch_idx, data in enumerate(train_loader):\n","        data = data.to(device)\n","        optimizer.zero_grad()\n","        output, _ = model(data)\n","        loss = loss_fn(output, data.y.view(-1,2).float().to(device))\n","        loss.backward()\n","        optimizer.step()\n","        avg_loss.append(loss.item())\n","        if batch_idx % log_interval == 0:\n","            print('Train epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch,\n","                                                                           batch_idx * len(data.x),\n","                                                                           len(train_loader.dataset),\n","                                                                           100. * batch_idx / len(train_loader),\n","                                                                           loss.item()))\n","    return sum(avg_loss)/len(avg_loss)\n","\n","def predicting(model, device, loader):\n","    model.eval()\n","    total_preds = torch.Tensor()\n","    total_labels = torch.Tensor()\n","    print('Make prediction for {} samples...'.format(len(loader.dataset)))\n","    with torch.no_grad():\n","        for data in loader:\n","            data = data.to(device)\n","            output, _ = model(data)\n","            total_preds = torch.cat((total_preds, output.cpu()), 0)\n","            total_labels = torch.cat((total_labels, data.y.view(-1, 2).cpu()), 0)\n","    return total_labels,total_preds\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xjvwP-GP-uQ4","executionInfo":{"status":"ok","timestamp":1616043549275,"user_tz":-420,"elapsed":28943,"user":{"displayName":"17020328 Trương Thành Huy","photoUrl":"","userId":"00098032910227394016"}},"outputId":"723eae92-9a67-4891-ee5c-6797a7a009e0"},"source":["lr = 0.001\n","num_epoch = 100\n","train_batch = 128\n","val_batch = 128\n","test_batch = 128 \n","log_interval = 20\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","#define model\n","n_class = 2\n","modeling = GINConvNet\n","model = modeling().to(device)\n","\n","#load model \n","model.load_state_dict(torch.load(pretrain_model_path))\n","# set_parameter_requires_grad(model)\n","# model.eval()\n","#add softmax\n","model.out = nn.Sequential(nn.Linear(128, 64), nn.ReLU(), nn.Linear(64, 32), nn.ReLU(), nn.Linear(32, n_class), nn.Softmax())\n","model = model.to(device)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cuda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PbgwsVTPs_BJ"},"source":["# test_data = TestbedDataset(root=data_path, dataset=dataset+'_test_mix')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vA9yMxYRtT4g"},"source":["#Train model"]},{"cell_type":"code","metadata":{"id":"R2FBcCsOPFww","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616043549279,"user_tz":-420,"elapsed":28933,"user":{"displayName":"17020328 Trương Thành Huy","photoUrl":"","userId":"00098032910227394016"}},"outputId":"400c8716-1013-4b36-cc0b-8aebd948150b"},"source":["model.eval()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GINConvNet(\n","  (dropout): Dropout(p=0.5, inplace=False)\n","  (relu): ReLU()\n","  (conv1): GINConv(nn=Sequential(\n","    (0): Linear(in_features=78, out_features=32, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=32, out_features=32, bias=True)\n","  ))\n","  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (conv2): GINConv(nn=Sequential(\n","    (0): Linear(in_features=32, out_features=32, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=32, out_features=32, bias=True)\n","  ))\n","  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (conv3): GINConv(nn=Sequential(\n","    (0): Linear(in_features=32, out_features=32, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=32, out_features=32, bias=True)\n","  ))\n","  (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (conv4): GINConv(nn=Sequential(\n","    (0): Linear(in_features=32, out_features=32, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=32, out_features=32, bias=True)\n","  ))\n","  (bn4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (conv5): GINConv(nn=Sequential(\n","    (0): Linear(in_features=32, out_features=32, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=32, out_features=32, bias=True)\n","  ))\n","  (bn5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (fc1_xd): Linear(in_features=32, out_features=128, bias=True)\n","  (embedding_xt_mut): Embedding(26, 128)\n","  (conv_xt_mut_1): Conv1d(1, 32, kernel_size=(8,), stride=(1,))\n","  (embedding_xt_meth): Embedding(26, 128)\n","  (conv_xt_meth_1): Conv1d(1, 32, kernel_size=(8,), stride=(1,))\n","  (pool_xt_mut_1): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n","  (conv_xt_mut_2): Conv1d(32, 64, kernel_size=(8,), stride=(1,))\n","  (pool_xt_mut_2): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n","  (conv_xt_mut_3): Conv1d(64, 128, kernel_size=(8,), stride=(1,))\n","  (pool_xt_mut_3): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n","  (fc1_xt_mut): Linear(in_features=1280, out_features=128, bias=True)\n","  (pool_xt_meth_1): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n","  (conv_xt_meth_2): Conv1d(32, 64, kernel_size=(8,), stride=(1,))\n","  (pool_xt_meth_2): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n","  (conv_xt_meth_3): Conv1d(64, 128, kernel_size=(8,), stride=(1,))\n","  (pool_xt_meth_3): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n","  (fc1_xt_meth): Linear(in_features=83584, out_features=128, bias=True)\n","  (fc1): Linear(in_features=384, out_features=1024, bias=True)\n","  (fc2): Linear(in_features=1024, out_features=128, bias=True)\n","  (out): Sequential(\n","    (0): Linear(in_features=128, out_features=64, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=64, out_features=32, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=32, out_features=2, bias=True)\n","    (5): Softmax(dim=None)\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"1r2OxVke-r9R"},"source":["print('Learning rate: ', lr)\n","print('Epochs: ', num_epoch)\n","feature_extract = True\n","model_st = \"ge_meth_cls\"\n","dataset = 'GDSC'\n","train_losses = []\n","val_losses = []\n","val_pearsons = []\n","print('\\nrunning on ', model_st + '_' + dataset )\n","processed_data_file_train = data_processed_path + dataset + '_train_mix'+'.pt'\n","processed_data_file_val = data_processed_path + dataset + '_val_mix'+'.pt'\n","processed_data_file_test = data_processed_path + dataset + '_test_mix'+'.pt'\n","if ((not os.path.isfile(processed_data_file_train)) or (not os.path.isfile(processed_data_file_val)) or (not os.path.isfile(processed_data_file_test))):\n","    print('please run create_data.py to prepare data in pytorch format!')\n","else:\n","    train_data = TestbedDataset(root=data_path, dataset=dataset+'_train_mix')\n","    val_data = TestbedDataset(root=data_path, dataset=dataset+'_val_mix')\n","    test_data = TestbedDataset(root=data_path, dataset=dataset+'_test_mix')\n","\n","    # make data PyTorch\n","    # mini-batch processing ready\n","    train_loader = DataLoader(train_data, batch_size=train_batch, shuffle=True)\n","    val_loader = DataLoader(val_data, batch_size=val_batch, shuffle=False)\n","    test_loader = DataLoader(test_data, batch_size=test_batch, shuffle=False)\n","    print(\"CPU/GPU: \", torch.cuda.is_available())\n","            \n","    # training the model\n","    params_to_update = model.parameters()\n","    print(\"Params to learn:\")\n","    if feature_extract:\n","        params_to_update = []\n","        for name,param in model.named_parameters():\n","            if param.requires_grad == True:\n","                params_to_update.append(param)\n","                print(\"\\t\",name)\n","    else:\n","        for name,param in model.named_parameters():\n","            if param.requires_grad == True:\n","                print(\"\\t\",name)\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    print(device)\n","    bce = nn.BCELoss()\n","    optimizer = torch.optim.Adam(params_to_update, lr=lr)    \n","    # best_bce = 1000\n","    # best_pearson = 1\n","    # best_epoch = -1\n","    # model_file_name = store_path+'model_' + model_st + '_' + dataset +  '.model'\n","    # result_file_name = store_path+'result_' + model_st + '_' + dataset +  '.csv'\n","    # loss_fig_name = store_path+'model_' + model_st + '_' + dataset + '_loss'\n","    # pearson_fig_name = store_path+'model_' + model_st + '_' + dataset + '_pearson'\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bPRF3ucGkqeQ"},"source":["best_bce = 1000\n","best_auc = 0\n","best_pearson = 1\n","best_epoch = -1\n","model_file_name = store_path+'model_' + model_st + '_' + dataset +  '.model'\n","result_file_name = store_path+'result_' + model_st + '_' + dataset +  '.csv'\n","loss_fig_name = store_path+'model_' + model_st + '_' + dataset + '_loss'\n","pearson_fig_name = store_path+'model_' + model_st + '_' + dataset + '_pearson'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OHev-10egrGB"},"source":["num_epoch = 200\n","best_ret_test = None\n","for epoch in range(num_epoch):\n","      train_loss = train(model, device, train_loader, optimizer, epoch+1, log_interval)\n","      G,P = predicting(model, device, val_loader)\n","      # G,P = predicting(model, device, test_loader)\n","\n","      ret = [roc_auc_score(G,P),bce(G,P),pearson(G,P),spearman(G,P)]\n","                  \n","      G_test,P_test = predicting(model, device, test_loader)\n","      ret_test = [roc_auc_score(G_test,P_test),bce(G_test,P_test),pearson(G_test,P_test),spearman(G_test,P_test)]\n","\n","      train_losses.append(train_loss)\n","      val_losses.append(ret[1])\n","      val_pearsons.append(ret[2])\n","      if ret[0]>best_auc:\n","          torch.save(model.state_dict(), model_file_name)\n","          with open(result_file_name,'w') as f:\n","              f.write(','.join(map(str,ret_test)))\n","          best_epoch = epoch+1\n","          best_bce = ret[1]\n","          best_pearson = ret[2]\n","          best_auc = ret[0]\n","          best_ret_test = ret_test\n","          print(' bce improved at epoch ', best_epoch, '; best_bce:', best_bce,model_st,dataset, \"; best_auc:\", best_auc)\n","      else:\n","          print(' no improvement since epoch ', best_epoch, '; best_bce, best pearson:', best_bce, best_pearson, model_st, dataset, \"; best_auc:\", best_auc)\n","draw_loss(train_losses, val_losses, loss_fig_name)\n","draw_pearson(val_pearsons, pearson_fig_name)\n","print(best_ret_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O5WOiLaFtV2O"},"source":["#Test on drugs"]},{"cell_type":"code","metadata":{"id":"sOro5apKCQvM"},"source":["pretrain_cls = \"/content/drive/MyDrive/05.New_DRP3_ge_meth/Pretrain_DRP/DRP_RS_ge_meth/model_ge_meth_cls_GDSC.model\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WBnNHZtdtZvH","executionInfo":{"status":"ok","timestamp":1616047220112,"user_tz":-420,"elapsed":659,"user":{"displayName":"17020328 Trương Thành Huy","photoUrl":"","userId":"00098032910227394016"}},"outputId":"7547c911-3fd2-4938-8a7b-49d3396a2ea1"},"source":["model.load_state_dict(torch.load(pretrain_cls))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"2gRWc4svterT"},"source":["data_path = \"/content/drive/MyDrive/05.New_DRP3_ge_meth/MOLI/preprocess/processed_ge_meth/\" # data to process"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jn3o_-EvgnRn"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rXavIlGIteAZ","executionInfo":{"status":"ok","timestamp":1616047227540,"user_tz":-420,"elapsed":4559,"user":{"displayName":"17020328 Trương Thành Huy","photoUrl":"","userId":"00098032910227394016"}},"outputId":"1989d9c3-0d13-45ab-91b9-f2c43c552d00"},"source":["GDSC_Docetaxel_test_mix = TestbedDataset(root=data_path, dataset=\"GDSC_Docetaxel_test_mix\")\n","GDSC_Erlotinib_test_mix = TestbedDataset(root=data_path, dataset=\"GDSC_Erlotinib_test_mix\")\n","GDSC_Gemcitabine_test_mix = TestbedDataset(root=data_path, dataset=\"GDSC_Gemcitabine_test_mix\")\n","GDSC_Paclitaxel_test_mix = TestbedDataset(root=data_path, dataset=\"GDSC_Paclitaxel_test_mix\")\n","\n","GDSC_Docetaxel_test_mix = DataLoader(GDSC_Docetaxel_test_mix, batch_size=test_batch, shuffle=False)\n","GDSC_Erlotinib_test_mix = DataLoader(GDSC_Erlotinib_test_mix, batch_size=test_batch, shuffle=False)\n","GDSC_Gemcitabine_test_mix = DataLoader(GDSC_Gemcitabine_test_mix, batch_size=test_batch, shuffle=False)\n","GDSC_Paclitaxel_test_mix = DataLoader(GDSC_Paclitaxel_test_mix, batch_size=test_batch, shuffle=False)\n","\n","test_loaders = [GDSC_Docetaxel_test_mix, GDSC_Erlotinib_test_mix, GDSC_Gemcitabine_test_mix, GDSC_Paclitaxel_test_mix]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Pre-processed data found: /content/drive/MyDrive/05.New_DRP3_ge_meth/MOLI/preprocess/processed_ge_meth/processed/GDSC_Docetaxel_test_mix.pt, loading ...\n","Pre-processed data found: /content/drive/MyDrive/05.New_DRP3_ge_meth/MOLI/preprocess/processed_ge_meth/processed/GDSC_Erlotinib_test_mix.pt, loading ...\n","Pre-processed data found: /content/drive/MyDrive/05.New_DRP3_ge_meth/MOLI/preprocess/processed_ge_meth/processed/GDSC_Gemcitabine_test_mix.pt, loading ...\n","Pre-processed data found: /content/drive/MyDrive/05.New_DRP3_ge_meth/MOLI/preprocess/processed_ge_meth/processed/GDSC_Paclitaxel_test_mix.pt, loading ...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2XdBJLifgYuh"},"source":["bce = nn.BCELoss()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lYNM6yYdwjs_"},"source":["for test_loader in test_loaders:\n","  G_test,P_test = predicting(model, device, test_loader)\n","  ret_test = [roc_auc_score(G_test,P_test),bce(G_test,P_test),pearson(G_test,P_test),spearman(G_test,P_test)]\n","  print(ret_test)"],"execution_count":null,"outputs":[]}]}